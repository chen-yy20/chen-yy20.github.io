---
layout: post
title: "游凯超：开源与vLLM之路"
date: 2025-10-25 20:19:00 +0800
author: "阳"
location: "三教3300"
tags: [HPCAI]
---

# 开源的世界和vLLM之路
三教3300

今天来到了TUNA组织的Tunight，追星游凯超学长。

Easy，Fast and Cheap。很普通却很实际，也很难。

## OpenSource

### 开源之路

16级本科，软院博。他在学校中读书，慢慢加入到开源的社区。
24年到UCB交换，在Ion Stoica指导下访学，参与PyTorch，vLLM项目
（PyTorch基金会：DeepSpeed, vLLM)

入门：Tensorflow中的一个typo，也需要pr的提交。

进阶：修功能，继承代码，版本维护和测试。为了一个小问题，可能关系到巨量代码。

> 世界破破烂烂，开发者缝缝补补。Welcome to open source.

精通：增加功能。集成入主流的框架。Motivation、Modification、接收以后要如何维护？

大成：成为核心维护者。考虑大局的规划和管理，比如feature的增加，贡献者如何保持继续贡献，与公司的交流，相互受益

### 开源的意义

信息/智能时代的基础设施构建在开源软件之上：

* Linux 操作系统
* k8s 云原生
* Spark 数据库
* PyTorch
* vLLM

没有充分测试过的场景，必然会出现问题：因为世界是一个巨大的草台班子，开源软件的维护者只会关心自身的使用场景，在次要的场景只会“尽力而为”，那么在开源社区中，就会出现有价值的场景，有价值的问题，也是在帮助未来的自己。

**社区**，对于我，是一个新的名词。

### 开源的挑战

钱不够，开源软件商业化成功的案例屈指可数。

人不够，纯粹用爱发电。而且**不出bug无人问津，一出问题举世皆知。** 还有人潜入进来藏bug。

模型的开源，已经以中国为主了。软件方面，也希望国内社区迎头赶上。



## vLLM

### Stories

起源：PagedAttention - UCB

高效组织管理KV Cache，提高显存利用率，从而增大batchsize，提高decode速度。

硬件厂商，云厂商，模型开发商。

目标：最快，最简单的推理引擎。offline 推理，也可以提供api作为服务器。

将Transformers作为vllm的前端，vllm支持transformers，从而更大程度地支持大部分模型。

vllm现在支持哪些模型：

Transformer-based, moe, state-space models, linear-attention 但是还不支持多模态和diffusion

Task: chat, embedding, reward, rerank

https://recipes.vllm.ai 常见的模型在常见的硬件下，怎样用vllm跑起来。

vllm需要和华为、寒武纪等各大硬件厂商制定硬件接口，实现多样化硬件支持。

多硬件支持离不开pytorch，因为硬件要首先支持torch才能考虑vllm。

> PyTorch就是一个连接多样硬件和模型的narrow waist。

因此vllm加入了PyTorch生态，vLLM的commit要经过pytorch最新版本测试，pytorch的版本发布也需要经过完成vllm的测试。

Blackwell架构需要cuda12.8，对pytorch和vllm都是问题。

### New Feature

* Semantic Router：模型路由

* LLM Compressor：模型量化

* llm-d：k8s集群上llm的大规模推理
* Hybrid-Allocator：Qwen3-Next（Jenga：异构LLM Serving的memory管理）

* KV Connector：提供一个KV Cache接口，将Cache与vllm解耦，管理交给其他项目
* Compiler-based optimization：太多模型，太多硬件，vllm作为集合体，如何避免太过臃肿？每加一个功能，都需要单独实现吗？需要从更加底层（编译）的角度做优化。
  * 以Sequence Parallel为例：通过Piecewise CUDA Graph，将attention交给researcher玩花活（灵活），其他per-token的模块用cuda graph包起来，追求极致的性能。Cuda graph消除cpu的overhead。
* Decode Context Parallel：with Chao Hong @ Moonshot，将kv cache也做切分，减少kv cache的冗余（tensor parallel的变种）
* SleepMode：训推一体，offload没用的cache。

* 预编译安装：很多人不需要改kernel，vLLM的编译过程非常的慢，可以直接拉取编译过的结果。
* 大规模ci：每月10w刀，每一个commit都需要ci。太多的依赖，根据diff去测试是很美好的想法，但其实并不安全。改了Qwen，却影响了DeepSeek。



当然这个项目很伟大，但已经不是当初的邻家小妹了，要学习最初的那篇论文中的vllm，可以参考：https://github.com/GeeeekExplorer/nano-vllm 。未来，vllm希望成为一个大模型推理的技术社区，所有与之相关的人，都在这里贡献自己的力量。



### QA

vllm本质上做两件事情：Continuous Batching和Paged Attention，能做这件事的模型就能进vLLM。

Diffusion 以及 DLM：不是这种模式，完全不适配vllm，与其加入vllm不如新开一个项目。
